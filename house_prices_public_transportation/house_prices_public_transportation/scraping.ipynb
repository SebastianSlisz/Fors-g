{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "disable_scraping = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get All bus routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping is disabled\n",
      "Number of unique bus lines: 424\n"
     ]
    }
   ],
   "source": [
    "if disable_scraping:\n",
    "    print('Scraping is disabled')\n",
    "    busRoutes = pd.read_pickle('data/raw/busRoutes.pkl')\n",
    "else:\n",
    "    url = 'https://dinoffentligetransport.dk/api/BusLines/lines'\n",
    "    response = requests.get(url,)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to get data:', response.status_code)\n",
    "        exit(1)\n",
    "    else:\n",
    "        # store the response in a pandas dataframe\n",
    "        busRoutes = pd.DataFrame(response.json())\n",
    "        busRoutes_list = busRoutes.designation.to_list()\n",
    "        busRoutes_refs = busRoutes.ref.to_list()\n",
    "\n",
    "    busRoutes.to_pickle('data/raw/busRoutes.pkl')\n",
    "\n",
    "print(f'Number of unique bus lines: {busRoutes.designation.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all busstops for each route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping is disabled\n"
     ]
    }
   ],
   "source": [
    "if disable_scraping:\n",
    "    print('Scraping is disabled')\n",
    "    with open('data/raw/busRoutes_dict.json', 'r') as f:\n",
    "        busroutes_dict = json.load(f)\n",
    "else:\n",
    "    busroutes_dict = {key:None for key in busRoutes_refs[:]}\n",
    "    for ref in list(busroutes_dict.keys()):\n",
    "        response = requests.get(f'https://dinoffentligetransport.dk/api/buslines/fromstoppoints/{ref}/1',)\n",
    "        busroutes_dict[ref] = response.json()\n",
    "        time.sleep(2)\n",
    "\n",
    "    with open('data/raw/busRoutes_dict.json', 'w') as f:\n",
    "        json.dump(busroutes_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_boligsiden(url:str, pagenumber:int):\n",
    "    # create header\n",
    "    headers={'name':'xkh771','mail':'xkh771@alumni.ku.dk'}\n",
    "    # intialize empty list to store data\n",
    "    dataList = []\n",
    "    # Looping through all available pages\n",
    "    for pagenumber in range(1,pagenumber):\n",
    "        # create a url with the page number\n",
    "        url_page = url + f'&page={pagenumber}'\n",
    "        # get the response\n",
    "        response = requests.get(url_page, headers=headers)\n",
    "        # load the json data\n",
    "        data = json.loads(response.text)\n",
    "        # append the data to the dataList\n",
    "        dataList.append(data) # Append fetched data to list\n",
    "        time.sleep(1) # Sleep for 1 second to avoid overloading the server\n",
    "\n",
    "    # Do the same for the other data\n",
    "    data_cases = [item['cases'] for item in dataList]\n",
    "    #Flatten the list\n",
    "    data_cases = [item for sublist in data_cases if sublist is not None for item in sublist]\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame(data_cases)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping is disabled\n"
     ]
    }
   ],
   "source": [
    "if disable_scraping:\n",
    "    print('Scraping is disabled')\n",
    "else:\n",
    "    cop_url = 'https://api.boligsiden.dk/search/cases?municipalities=K%C3%B8benhavn&municipalities=Frederiksberg&addressTypes=villa%2Ccondo%2Cterraced+house%2Choliday+house%2Ccooperative%2Cfarm%2Chobby+farm%2Cfull+year+plot%2Cvilla+apartment%2Choliday+plot%2Chouseboat&per_page=50&highlighted=false&sortBy=random'\n",
    "    omegn_url = 'https://api.boligsiden.dk/search/cases?provinces=K%C3%B8benhavns+omegn&addressTypes=villa%2Ccondo%2Cterraced+house%2Choliday+house%2Ccooperative%2Cfarm%2Chobby+farm%2Cfull+year+plot%2Cvilla+apartment%2Choliday+plot%2Chouseboat&per_page=50&highlighted=false&sortBy=random'\n",
    "    df_cop = scrape_boligsiden(cop_url, 43)\n",
    "    df_omegn_data = scrape_boligsiden(omegn_url, 49)\n",
    "    df_raw_data = pd.concat([df_cop, df_omegn_data], axis=0)\n",
    "    df_raw_data.to_pickle('data/raw/df_raw_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metro stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping is disabled\n",
      "Number of unique metro stations: 44\n"
     ]
    }
   ],
   "source": [
    "## Metro station\n",
    "if disable_scraping:\n",
    "    print('Scraping is disabled')\n",
    "    df_metro = pd.read_pickle('data/raw/metro.pkl')\n",
    "else:\n",
    "   url = 'https://m.dk/'\n",
    "   response = requests.get(url, headers={'name':'xkh771','mail':'xkh71@alumni.ku.dk'})\n",
    "   soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "   svg_element = soup.find('svg', {'id':'cont'})\n",
    "   g_elements = svg_element.find_all('g')\n",
    "   metro_stations = []\n",
    "   for g in g_elements:\n",
    "      a_tag = g.find('a')\n",
    "      if a_tag:\n",
    "         text = a_tag.text.strip()\n",
    "         if text and text[0].isalpha():\n",
    "            metro_stations.append(text)\n",
    "\n",
    "   cleaned_text = [x.replace('\\n',' ') for x in metro_stations]\n",
    "   df_metro = pd.DataFrame(cleaned_text, columns=['MetroStation'])\n",
    "   df_metro.to_pickle('data/raw/metro.pkl')\n",
    "\n",
    "print(f'Number of unique metro stations: {df_metro.MetroStation.nunique()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
